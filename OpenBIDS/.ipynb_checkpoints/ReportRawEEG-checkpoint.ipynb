{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "64d9ea56-43da-4708-8c48-c1a24b4d0cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "import cmlreaders as cml\n",
    "from cmldask import CMLDask as da\n",
    "from dask.distributed import wait, as_completed\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.colors as mcolors\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import scipy as scp\n",
    "import re\n",
    "from scipy import stats\n",
    "from ptsa.data.timeseries import *\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import pyedflib\n",
    "from mne_bids import get_entity_vals\n",
    "from ReportRawEEG import *\n",
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.max_columns = 50\n",
    "%matplotlib inline\n",
    "import mne\n",
    "from mne_bids import BIDSPath, read_raw_bids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "195267ae-7060-4ad8-89d0-8d01635479a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "def fix_evs_bids(full_evs):\n",
    "    value_recalls = full_evs[full_evs.trial_type == \"VALUE_RECALL\"] \n",
    "    words = full_evs[full_evs.trial_type == \"WORD\"]\n",
    "    rec_words = full_evs[full_evs.trial_type == \"REC_WORD\"]\n",
    "    rec_vv_words = full_evs[full_evs.trial_type == \"REC_WORD_VV\"]\n",
    "\n",
    "    # WORD --> storepointtype, recalled--> VALUE_RECALL, REC_WORD, REC_WORD_VV\n",
    "    word_trial_to_storepointtype = words.set_index(\"trial\")[\"storepointtype\"].to_dict()\n",
    "    word_trial_to_recalled = words.set_index(\"trial\")[\"recalled\"].to_dict()\n",
    "    for event_type in [\"VALUE_RECALL\", \"REC_WORD\", \"REC_WORD_VV\"]:\n",
    "        subset = full_evs[full_evs.trial_type == event_type]\n",
    "        for idx, row in subset.iterrows():\n",
    "            trial = row[\"trial\"]\n",
    "            if trial in word_trial_to_storepointtype:\n",
    "                full_evs.at[idx, \"storepointtype\"] = word_trial_to_storepointtype[trial]\n",
    "            if trial in word_trial_to_recalled:\n",
    "                full_evs.at[idx, \"recalled\"] = word_trial_to_recalled[trial]\n",
    "\n",
    "    # VALUE_RECALL --> actualvalue, valuerecall --> WORD, `REC_WORD`, REC_WORD_VV\n",
    "    valuerecall_trial_to_actualvalue = value_recalls.set_index(\"trial\")[\"actualvalue\"].to_dict()\n",
    "    valuerecall_trial_to_valuerecall = value_recalls.set_index(\"trial\")[\"valuerecall\"].to_dict()\n",
    "\n",
    "    # --- Apply to multi-row event types ---\n",
    "    for event_type in [\"WORD\", \"REC_WORD\", \"REC_WORD_VV\"]:\n",
    "        subset = full_evs[full_evs.trial_type == event_type]\n",
    "        for idx, row in subset.iterrows():\n",
    "            trial = row[\"trial\"]\n",
    "\n",
    "            # actualvalue\n",
    "            if trial in valuerecall_trial_to_actualvalue:\n",
    "                full_evs.at[idx, \"actualvalue\"] = valuerecall_trial_to_actualvalue[trial]\n",
    "\n",
    "            # valuerecall\n",
    "            if trial in valuerecall_trial_to_valuerecall:\n",
    "                full_evs.at[idx, \"valuerecall\"] = valuerecall_trial_to_valuerecall[trial]\n",
    "                \n",
    "    return full_evs\n",
    "\n",
    "def fix_evs_cml(full_evs):\n",
    "    value_recalls = full_evs[full_evs.type == \"VALUE_RECALL\"] \n",
    "    words = full_evs[full_evs.type == \"WORD\"]\n",
    "    rec_words = full_evs[full_evs.type == \"REC_WORD\"]\n",
    "    rec_vv_words = full_evs[full_evs.type == \"REC_WORD_VV\"]\n",
    "\n",
    "    # WORD --> storepointtype, recalled--> VALUE_RECALL, REC_WORD, REC_WORD_VV\n",
    "    word_trial_to_storepointtype = words.set_index(\"trial\")[\"storepointtype\"].to_dict()\n",
    "    word_trial_to_recalled = words.set_index(\"trial\")[\"recalled\"].to_dict()\n",
    "    for event_type in [\"VALUE_RECALL\", \"REC_WORD\", \"REC_WORD_VV\"]:\n",
    "        subset = full_evs[full_evs.type == event_type]\n",
    "        for idx, row in subset.iterrows():\n",
    "            trial = row[\"trial\"]\n",
    "            if trial in word_trial_to_storepointtype:\n",
    "                full_evs.at[idx, \"storepointtype\"] = word_trial_to_storepointtype[trial]\n",
    "            if trial in word_trial_to_recalled:\n",
    "                full_evs.at[idx, \"recalled\"] = word_trial_to_recalled[trial]\n",
    "\n",
    "    # VALUE_RECALL --> actualvalue, valuerecall --> WORD, `REC_WORD`, REC_WORD_VV\n",
    "    valuerecall_trial_to_actualvalue = value_recalls.set_index(\"trial\")[\"actualvalue\"].to_dict()\n",
    "    valuerecall_trial_to_valuerecall = value_recalls.set_index(\"trial\")[\"valuerecall\"].to_dict()\n",
    "\n",
    "    # --- Apply to multi-row event types ---\n",
    "    for event_type in [\"WORD\", \"REC_WORD\", \"REC_WORD_VV\"]:\n",
    "        subset = full_evs[full_evs.type == event_type]\n",
    "        for idx, row in subset.iterrows():\n",
    "            trial = row[\"trial\"]\n",
    "\n",
    "            # actualvalue\n",
    "            if trial in valuerecall_trial_to_actualvalue:\n",
    "                full_evs.at[idx, \"actualvalue\"] = valuerecall_trial_to_actualvalue[trial]\n",
    "\n",
    "            # valuerecall\n",
    "            if trial in valuerecall_trial_to_valuerecall:\n",
    "                full_evs.at[idx, \"valuerecall\"] = valuerecall_trial_to_valuerecall[trial]\n",
    "                \n",
    "    return full_evs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9f47b2d4-1ff8-44a4-93c1-7e3f39e2533b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### ACROSS MULTIPLE SUBJECTS AND SESSIONS\n",
    "bids_root = \"/home1/maint/LTP_BIDS/\"\n",
    "subjects = get_entity_vals(bids_root, \"subject\")\n",
    "\n",
    "\n",
    "# subject\n",
    "def process_raw_signals(sub, exp, sess, bids_root, out_path): # entire signal, not epoched\n",
    "    ### load cml\n",
    "    reader = cml.CMLReader(subject=sub, experiment=exp, session=sess)\n",
    "    eeg_cml = reader.load_eeg().to_ptsa()\n",
    "\n",
    "    ### load bdf\n",
    "    # BIDS\n",
    "    bids_path = BIDSPath(\n",
    "        subject=sub,\n",
    "        session=str(sess),\n",
    "        task=exp.lower(),\n",
    "        datatype=\"eeg\",\n",
    "        root=bids_root,\n",
    "    )\n",
    "\n",
    "    raw = read_raw_bids(\n",
    "        bids_path,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    eeg_bids = xr.DataArray(\n",
    "        raw.get_data()[None, :, :],                           # -> (1, n_channels, n_times)\n",
    "        dims=(\"event\", \"channel\", \"time\"),         # match eeg_cml dim names\n",
    "        coords={\n",
    "            \"event\": [0],                          # singleton event index\n",
    "            \"channel\": raw.ch_names,\n",
    "            \"time\": raw.times * 1000,\n",
    "            \"samplerate\": raw.info[\"sfreq\"],                    # scalar coord (optional)\n",
    "        },\n",
    "        name=\"eeg\",\n",
    "    )\n",
    "\n",
    "    ## load pyedf\n",
    "    # cml_bdf_path  = f\"/protocols/ltp/subjects/{sub}/experiments/{exp}/sessions/{sess}/ephys/current_processed/{sub}_session_{sess}.bdf\"\n",
    "    # eeg_pyedflib = load_bdf_as_xarray(cml_bdf_path)\n",
    "\n",
    "    # compare\n",
    "    results = compare_eeg_sources(\n",
    "        eeg_dict={\"BIDS\": eeg_bids, \"CMLReader\": eeg_cml},\n",
    "        subject=sub,\n",
    "        experiment=exp,\n",
    "        session=sess,\n",
    "        options=[\"strip_metadata\", \"compare_raw_signals\", \"compare_time_coords\"]\n",
    "    )\n",
    "    \n",
    "    results[\"df_raw\"].to_csv(f\"{out_path}df_raw_{sub}_{exp}_{sess}.csv\", index=False)\n",
    "    results[\"df_raw_summary\"].to_csv(f\"{out_path}df_raw_summary_{sub}_{exp}_{sess}.csv\", index=False)\n",
    "    results[\"df_time\"].to_csv(f\"{out_path}df_time_{sub}_{exp}_{sess}.csv\", index=False)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "45355f54-4e98-4b45-8c3a-541d7555dd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _all_exist(paths):\n",
    "    return all(os.path.exists(p) for p in paths)\n",
    "\n",
    "def load_bids_events(sub, exp, sess, bids_root):\n",
    "    \"\"\"\n",
    "    Load BIDS events file, trying eeg datatype first, then beh datatype.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame or None\n",
    "        Events dataframe if found, None otherwise\n",
    "    \"\"\"\n",
    "    exp_lower = exp.lower()\n",
    "    \n",
    "    # Try eeg datatype first\n",
    "    try:\n",
    "        bids_path = BIDSPath(\n",
    "            subject=sub,\n",
    "            session=str(sess),\n",
    "            task=exp_lower,\n",
    "            datatype=\"eeg\",\n",
    "            root=bids_root,\n",
    "        )\n",
    "        events_path = os.path.join(bids_path.directory, bids_path.basename + \"_events.tsv\")\n",
    "        evs_bids = pd.read_csv(events_path, sep=\"\\t\")\n",
    "        print(f\"Loaded events from eeg datatype: {events_path}\")\n",
    "        return evs_bids\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {sub} | {exp} | {sess} events from eeg folder: {e}\")\n",
    "    \n",
    "    # Try beh datatype as fallback\n",
    "    try:\n",
    "        bids_path = BIDSPath(\n",
    "            subject=sub,\n",
    "            session=str(sess),\n",
    "            task=exp_lower,\n",
    "            datatype=\"eeg\",\n",
    "            suffix=\"beh\",\n",
    "            extension=\".tsv\",\n",
    "            root=bids_root\n",
    "        )\n",
    "        evs_bids = pd.read_csv(bids_path.fpath, sep=\"\\t\")\n",
    "        print(f\"Loaded events from beh datatype: {bids_path.fpath}\")\n",
    "        return evs_bids\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {sub} | {exp} | {sess} events from beh folder: {e}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "def process_events(sub, exp, sess, evs_types, bids_root, out_path, *, skip_if_exists=True):\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "    out_behavior_summary = os.path.join(out_path, f\"df_behavior_summary_{sub}_{exp}_{sess}.csv\")\n",
    "    \n",
    "    expected = [out_behavior_summary]\n",
    "    if skip_if_exists and _all_exist(expected):\n",
    "        return {\"skipped\": True, \"reason\": \"outputs_exist\", \"paths\": expected}\n",
    "    \n",
    "    # Load CML events\n",
    "    try:\n",
    "        cmlreader = cml.CMLReader(subject=sub, experiment=exp, session=sess)\n",
    "        evs_cml = cmlreader.load('events')\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load CML events for {sub} | {exp} | {sess}: {e}\")\n",
    "        return {\"skipped\": True, \"reason\": \"cml_load_failed\", \"error\": str(e)}\n",
    "    \n",
    "    evs_types_set = set(evs_types) if evs_types is not None else set(evs_cml[\"type\"].unique())\n",
    "    \n",
    "    if exp == \"ValueCourier\":\n",
    "        evs_cml = fix_evs_cml(evs_cml)\n",
    "    \n",
    "    filtered_evs_cml = evs_cml[evs_cml[\"type\"].isin(evs_types_set)]\n",
    "    \n",
    "    # Load BIDS events\n",
    "    evs_bids = load_bids_events(sub, exp, sess, bids_root)\n",
    "    \n",
    "    if evs_bids is None:\n",
    "        print(f\"Skipping {sub} | {exp} | {sess}: BIDS events file not found\")\n",
    "        return {\"skipped\": True, \"reason\": \"bids_events_not_found\"}\n",
    "    \n",
    "    if exp == \"ValueCourier\":\n",
    "        evs_bids = fix_evs_bids(evs_bids)\n",
    "    \n",
    "    # Check for required columns\n",
    "    required_cols = {'sample', 'onset', 'trial_type'}\n",
    "    missing_cols = required_cols - set(evs_bids.columns)\n",
    "    if missing_cols:\n",
    "        print(f\"Skipping {sub} | {exp} | {sess}: BIDS events missing required columns: {missing_cols}\")\n",
    "        print(f\"Available columns: {list(evs_bids.columns)}\")\n",
    "        return {\"skipped\": True, \"reason\": \"missing_columns\", \"missing\": list(missing_cols)}\n",
    "    \n",
    "    filtered_evs_bids = evs_bids[evs_bids[\"trial_type\"].isin(evs_types_set)]\n",
    "    \n",
    "    if filtered_evs_bids.empty:\n",
    "        print(f\"Skipping {sub} | {exp} | {sess}: No events match the requested types\")\n",
    "        return {\"skipped\": True, \"reason\": \"no_matching_events\"}\n",
    "    \n",
    "    # Compare behavioral data\n",
    "    try:\n",
    "        results = compare_behavioral(\n",
    "            filtered_evs_cml, \"CMLReader\",\n",
    "            filtered_evs_bids, \"OpenBIDS\",\n",
    "            options=[\n",
    "                \"compare_onset_as_diff\",\n",
    "                \"tolerant_numeric\",\n",
    "                \"return_col_summary\",\n",
    "                \"return_mismatches\",\n",
    "            ],\n",
    "            drop_cols=[],\n",
    "        )\n",
    "        \n",
    "        os.makedirs(out_path, exist_ok=True)\n",
    "        results[\"df_behavior_summary\"].to_csv(\n",
    "            os.path.join(out_path, f\"df_behavior_summary_{sub}_{exp}_{sess}.csv\"),\n",
    "            index=False,\n",
    "        )\n",
    "        \n",
    "        print(f\"Successfully processed {sub} | {exp} | {sess}\")\n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to compare events for {sub} | {exp} | {sess}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return {\"skipped\": True, \"reason\": \"comparison_failed\", \"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d8ff62df-848d-41b3-ae69-fad32fc6d9bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# process_events(\"LTP606\", \"ValueCourier\", 0, None, \"/home1/maint/LTP_BIDS/\", \"raw_results/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5e0971bf-7374-4320-acb8-b819e6ac92d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne_bids import BIDSPath\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "from ReportRawEEG import *\n",
    "\n",
    "def _all_exist(paths):\n",
    "    return all(os.path.exists(p) for p in paths)\n",
    "\n",
    "def _dedupe_events_by_sample(df: pd.DataFrame, sample_col: str, *, keep=\"first\") -> pd.DataFrame:\n",
    "    if sample_col not in df.columns:\n",
    "        raise ValueError(f\"Expected column '{sample_col}' in events df. Columns={list(df.columns)[:20]}\")\n",
    "    df2 = df.copy()\n",
    "    df2[sample_col] = pd.to_numeric(df2[sample_col], errors=\"coerce\")\n",
    "    df2 = df2.dropna(subset=[sample_col])\n",
    "    df2 = df2.sort_values(sample_col, kind=\"mergesort\")\n",
    "    df2 = df2[~df2[sample_col].duplicated(keep=keep)]\n",
    "    return df2\n",
    "\n",
    "def _as_list(x):\n",
    "    if x is None:\n",
    "        return None\n",
    "    if isinstance(x, (list, tuple, set, np.ndarray, pd.Index)):\n",
    "        return list(x)\n",
    "    return [x]\n",
    "\n",
    "def process_epoched_signals_by_type(\n",
    "    sub,\n",
    "    exp,\n",
    "    sess,\n",
    "    evs_types,\n",
    "    tmin,\n",
    "    tmax,\n",
    "    bids_root,\n",
    "    out_path,\n",
    "    *,\n",
    "    skip_if_exists=True,\n",
    "    keep=\"first\",\n",
    "    verbose=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Run epoch+compare separately for each event type, append results across types,\n",
    "    save and return the appended DataFrames.\n",
    "    \"\"\"\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "    # aggregated outputs (ONE set per sub/exp/sess)\n",
    "    out_raw = os.path.join(out_path, f\"df_raw_{sub}_{exp}_{sess}.csv\")\n",
    "    out_raw_summary = os.path.join(out_path, f\"df_raw_summary_{sub}_{exp}_{sess}.csv\")\n",
    "    out_time = os.path.join(out_path, f\"df_time_{sub}_{exp}_{sess}.csv\")\n",
    "    expected = [out_raw, out_raw_summary, out_time]\n",
    "\n",
    "    if skip_if_exists and _all_exist(expected):\n",
    "        print(\"Files exist: skipped\")\n",
    "        return {\"skipped\": True, \"reason\": \"outputs_exist\", \"paths\": expected}\n",
    "\n",
    "    # --------------------------\n",
    "    # CML: load events once\n",
    "    # --------------------------\n",
    "    cmlreader = cml.CMLReader(subject=sub, experiment=exp, session=sess)\n",
    "    evs_cml = cmlreader.load(\"events\")\n",
    "\n",
    "    # decide which types to run\n",
    "    if evs_types is None:\n",
    "        types_to_run = sorted(pd.unique(evs_cml[\"type\"]))\n",
    "    else:\n",
    "        types_to_run = sorted(set(_as_list(evs_types)))\n",
    "\n",
    "    if len(types_to_run) == 0:\n",
    "        raise ValueError(\"types_to_run is empty.\")\n",
    "\n",
    "    # --------------------------\n",
    "    # BIDS: load raw + annotations once\n",
    "    # --------------------------\n",
    "    task = exp.lower()\n",
    "    bids_path = BIDSPath(\n",
    "        subject=sub,\n",
    "        session=str(sess),\n",
    "        task=task,\n",
    "        datatype=\"eeg\",\n",
    "        root=bids_root,\n",
    "    )\n",
    "\n",
    "    raw_bids = read_raw_bids(bids_path)\n",
    "    raw_bids.set_channel_types({\n",
    "        \"EXG1\": \"eog\", \"EXG2\": \"eog\", \"EXG3\": \"eog\", \"EXG4\": \"eog\",\n",
    "        \"EXG5\": \"misc\", \"EXG6\": \"misc\", \"EXG7\": \"misc\", \"EXG8\": \"misc\",\n",
    "    })\n",
    "\n",
    "    events_all, event_id_all = mne.events_from_annotations(raw_bids)\n",
    "    sfreq = float(raw_bids.info[\"sfreq\"])\n",
    "\n",
    "    # collect per-type outputs\n",
    "    all_raw = []\n",
    "    all_raw_summary = []\n",
    "    all_time = []\n",
    "\n",
    "    # optional bookkeeping\n",
    "    per_type_status = []\n",
    "\n",
    "    for etype in types_to_run:\n",
    "        if verbose:\n",
    "            print(f\"[{sub} | {exp} | {sess}] type={etype}\")\n",
    "\n",
    "        try:\n",
    "            # --------------------------\n",
    "            # CML: filter to this type + dedupe by eegoffset, then epoch\n",
    "            # --------------------------\n",
    "            evs_cml_t = evs_cml[evs_cml[\"type\"] == etype].copy()\n",
    "            if evs_cml_t.empty:\n",
    "                per_type_status.append((etype, \"skip\", \"no_cml_events\"))\n",
    "                continue\n",
    "\n",
    "            evs_cml_t = _dedupe_events_by_sample(evs_cml_t, \"eegoffset\", keep=keep)\n",
    "\n",
    "            eeg_cml = cmlreader.load_eeg(evs_cml_t, rel_start=tmin, rel_stop=tmax).to_ptsa()\n",
    "\n",
    "            # --------------------------\n",
    "            # BIDS: filter annotation labels/codes for this type, dedupe by sample, epoch\n",
    "            # --------------------------\n",
    "            if etype not in event_id_all:\n",
    "                per_type_status.append((etype, \"skip\", \"etype_not_in_annotations\"))\n",
    "                # free CML epoch before continue\n",
    "                del eeg_cml\n",
    "                gc.collect()\n",
    "                continue\n",
    "\n",
    "            filtered_event_id = {etype: event_id_all[etype]}\n",
    "            code = filtered_event_id[etype]\n",
    "\n",
    "            events_filt = events_all[events_all[:, 2] == code]\n",
    "            if len(events_filt) == 0:\n",
    "                per_type_status.append((etype, \"skip\", \"no_bids_events\"))\n",
    "                del eeg_cml\n",
    "                gc.collect()\n",
    "                continue\n",
    "\n",
    "            # dedupe by sample\n",
    "            _, first_idx = np.unique(events_filt[:, 0], return_index=True)\n",
    "            events_filt = events_filt[np.sort(first_idx)]\n",
    "\n",
    "            epochs_bids = mne.Epochs(\n",
    "                raw_bids,\n",
    "                events=events_filt,\n",
    "                event_id=filtered_event_id,\n",
    "                tmin=tmin / 1000.0,\n",
    "                tmax=tmax / 1000.0,\n",
    "                baseline=None,\n",
    "                preload=True,\n",
    "            )\n",
    "\n",
    "            picks_eeg = mne.pick_types(epochs_bids.info, eeg=True, eog=False, misc=False)\n",
    "            epochs_bids = epochs_bids.pick(picks_eeg)\n",
    "\n",
    "            # metadata aligned to events_filt\n",
    "            meta = pd.DataFrame({\n",
    "                \"sample\": events_filt[:, 0].astype(int),\n",
    "                \"trial_type\": [etype] * len(events_filt),\n",
    "            })\n",
    "            meta[\"onset\"] = meta[\"sample\"] / sfreq\n",
    "\n",
    "            eeg_bids = TimeSeries.from_mne_epochs(epochs_bids, meta)\n",
    "            eeg_bids = eeg_bids.assign_coords(time=eeg_bids[\"time\"] * 1000.0)\n",
    "            eeg_bids[\"time\"].attrs[\"units\"] = \"ms\"\n",
    "\n",
    "            # --------------------------\n",
    "            # Compare\n",
    "            # --------------------------\n",
    "            res = compare_eeg_sources(\n",
    "                eeg_dict={\"BIDS\": eeg_bids, \"CMLReader\": eeg_cml},\n",
    "                subject=sub,\n",
    "                experiment=exp,\n",
    "                session=sess,\n",
    "                options=[\"strip_metadata\", \"compare_raw_signals\", \"compare_time_coords\"],\n",
    "            )\n",
    "\n",
    "            # append dfs; add event type column so you can stratify later\n",
    "            if res.get(\"df_raw\") is not None and not res[\"df_raw\"].empty:\n",
    "                df = res[\"df_raw\"].copy()\n",
    "                df[\"event_type\"] = etype\n",
    "                all_raw.append(df)\n",
    "\n",
    "            if res.get(\"df_raw_summary\") is not None and not res[\"df_raw_summary\"].empty:\n",
    "                df = res[\"df_raw_summary\"].copy()\n",
    "                df[\"event_type\"] = etype\n",
    "                all_raw_summary.append(df)\n",
    "\n",
    "            if res.get(\"df_time\") is not None and not res[\"df_time\"].empty:\n",
    "                df = res[\"df_time\"].copy()\n",
    "                df[\"event_type\"] = etype\n",
    "                all_time.append(df)\n",
    "\n",
    "            per_type_status.append((etype, \"ok\", \"\"))\n",
    "\n",
    "        except Exception as e:\n",
    "            per_type_status.append((etype, \"fail\", repr(e)))\n",
    "\n",
    "        finally:\n",
    "            # free big objects per type\n",
    "            for name in (\"epochs_bids\", \"eeg_bids\", \"eeg_cml\", \"res\", \"events_filt\", \"meta\"):\n",
    "                if name in locals():\n",
    "                    try:\n",
    "                        del locals()[name]\n",
    "                    except Exception:\n",
    "                        pass\n",
    "            gc.collect()\n",
    "\n",
    "    # done with BIDS raw\n",
    "    try:\n",
    "        raw_bids.close()\n",
    "    except Exception:\n",
    "        pass\n",
    "    del raw_bids\n",
    "    gc.collect()\n",
    "\n",
    "    # concatenate and save\n",
    "    df_raw_all = pd.concat(all_raw, ignore_index=True) if all_raw else pd.DataFrame()\n",
    "    df_raw_summary_all = pd.concat(all_raw_summary, ignore_index=True) if all_raw_summary else pd.DataFrame()\n",
    "    df_time_all = pd.concat(all_time, ignore_index=True) if all_time else pd.DataFrame()\n",
    "\n",
    "    df_raw_all.to_csv(out_raw, index=False)\n",
    "    df_raw_summary_all.to_csv(out_raw_summary, index=False)\n",
    "    df_time_all.to_csv(out_time, index=False)\n",
    "\n",
    "    return {\n",
    "        \"df_raw\": df_raw_all,\n",
    "        \"df_raw_summary\": df_raw_summary_all,\n",
    "        \"df_time\": df_time_all,\n",
    "        \"per_type_status\": pd.DataFrame(per_type_status, columns=[\"event_type\", \"status\", \"detail\"]),\n",
    "        \"paths\": expected,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "163e54aa-2cc4-4e5f-824c-fd33d94e66bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique port for zrentala is 51618\n",
      "{'dashboard_address': ':51618'}\n",
      "To view the dashboard, run: \n",
      "`ssh -fN zrentala@rhino2.psych.upenn.edu -L 8000:192.168.86.140:38422` in your local computer's terminal (NOT rhino) \n",
      "and then navigate to localhost:8000 in your browser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/global/miniconda/py310_23.1.0-1/envs/workshop_311/lib/python3.11/site-packages/distributed/node.py:182: UserWarning: Port 51618 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 38422 instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "client = da.new_dask_client_slurm(\n",
    "    job_name=\"raw_signals\",\n",
    "    memory_per_job=\"100GB\",\n",
    "    max_n_jobs=20,\n",
    "    queue=\"RAM\",\n",
    "    local_directory=\"~/scratch\",\n",
    "    log_directory=os.path.expanduser(\"~/log_directory\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "31fd109b-7922-4848-a344-e5d4490766d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recognition</th>\n",
       "      <th>all_events</th>\n",
       "      <th>contacts</th>\n",
       "      <th>experiment</th>\n",
       "      <th>import_type</th>\n",
       "      <th>localization</th>\n",
       "      <th>math_events</th>\n",
       "      <th>montage</th>\n",
       "      <th>original_experiment</th>\n",
       "      <th>original_session</th>\n",
       "      <th>pairs</th>\n",
       "      <th>ps4_events</th>\n",
       "      <th>session</th>\n",
       "      <th>subject</th>\n",
       "      <th>subject_alias</th>\n",
       "      <th>system_version</th>\n",
       "      <th>task_events</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>protocols/ltp/subjects/LTP229/experiments/VFFR...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>VFFR</td>\n",
       "      <td>build</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>LTP229</td>\n",
       "      <td>LTP229</td>\n",
       "      <td>NaN</td>\n",
       "      <td>protocols/ltp/subjects/LTP229/experiments/VFFR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>protocols/ltp/subjects/LTP229/experiments/VFFR...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>VFFR</td>\n",
       "      <td>build</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>LTP229</td>\n",
       "      <td>LTP229</td>\n",
       "      <td>NaN</td>\n",
       "      <td>protocols/ltp/subjects/LTP229/experiments/VFFR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>protocols/ltp/subjects/LTP229/experiments/VFFR...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>VFFR</td>\n",
       "      <td>build</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>LTP229</td>\n",
       "      <td>LTP229</td>\n",
       "      <td>NaN</td>\n",
       "      <td>protocols/ltp/subjects/LTP229/experiments/VFFR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>protocols/ltp/subjects/LTP229/experiments/VFFR...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>VFFR</td>\n",
       "      <td>build</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>LTP229</td>\n",
       "      <td>LTP229</td>\n",
       "      <td>NaN</td>\n",
       "      <td>protocols/ltp/subjects/LTP229/experiments/VFFR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>protocols/ltp/subjects/LTP229/experiments/VFFR...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>VFFR</td>\n",
       "      <td>build</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>LTP229</td>\n",
       "      <td>LTP229</td>\n",
       "      <td>NaN</td>\n",
       "      <td>protocols/ltp/subjects/LTP229/experiments/VFFR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>NaN</td>\n",
       "      <td>protocols/ltp/subjects/LTP610/experiments/Valu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ValueCourier</td>\n",
       "      <td>build</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>LTP610</td>\n",
       "      <td>LTP610</td>\n",
       "      <td>NaN</td>\n",
       "      <td>protocols/ltp/subjects/LTP610/experiments/Valu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>NaN</td>\n",
       "      <td>protocols/ltp/subjects/LTP610/experiments/Valu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ValueCourier</td>\n",
       "      <td>build</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>LTP610</td>\n",
       "      <td>LTP610</td>\n",
       "      <td>NaN</td>\n",
       "      <td>protocols/ltp/subjects/LTP610/experiments/Valu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>NaN</td>\n",
       "      <td>protocols/ltp/subjects/LTP612/experiments/Valu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ValueCourier</td>\n",
       "      <td>build</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>LTP612</td>\n",
       "      <td>LTP612</td>\n",
       "      <td>NaN</td>\n",
       "      <td>protocols/ltp/subjects/LTP612/experiments/Valu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>NaN</td>\n",
       "      <td>protocols/ltp/subjects/LTP613/experiments/Valu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ValueCourier</td>\n",
       "      <td>build</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>LTP613</td>\n",
       "      <td>LTP613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>protocols/ltp/subjects/LTP613/experiments/Valu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>NaN</td>\n",
       "      <td>protocols/ltp/subjects/LTP614/experiments/Valu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ValueCourier</td>\n",
       "      <td>build</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>LTP614</td>\n",
       "      <td>LTP614</td>\n",
       "      <td>NaN</td>\n",
       "      <td>protocols/ltp/subjects/LTP614/experiments/Valu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Recognition                                         all_events contacts  \\\n",
       "0           NaN  protocols/ltp/subjects/LTP229/experiments/VFFR...      NaN   \n",
       "1           NaN  protocols/ltp/subjects/LTP229/experiments/VFFR...      NaN   \n",
       "2           NaN  protocols/ltp/subjects/LTP229/experiments/VFFR...      NaN   \n",
       "3           NaN  protocols/ltp/subjects/LTP229/experiments/VFFR...      NaN   \n",
       "4           NaN  protocols/ltp/subjects/LTP229/experiments/VFFR...      NaN   \n",
       "..          ...                                                ...      ...   \n",
       "109         NaN  protocols/ltp/subjects/LTP610/experiments/Valu...      NaN   \n",
       "110         NaN  protocols/ltp/subjects/LTP610/experiments/Valu...      NaN   \n",
       "111         NaN  protocols/ltp/subjects/LTP612/experiments/Valu...      NaN   \n",
       "112         NaN  protocols/ltp/subjects/LTP613/experiments/Valu...      NaN   \n",
       "113         NaN  protocols/ltp/subjects/LTP614/experiments/Valu...      NaN   \n",
       "\n",
       "       experiment import_type  localization math_events  montage  \\\n",
       "0            VFFR       build             0         NaN        0   \n",
       "1            VFFR       build             0         NaN        0   \n",
       "2            VFFR       build             0         NaN        0   \n",
       "3            VFFR       build             0         NaN        0   \n",
       "4            VFFR       build             0         NaN        0   \n",
       "..            ...         ...           ...         ...      ...   \n",
       "109  ValueCourier       build             0         NaN        0   \n",
       "110  ValueCourier       build             0         NaN        0   \n",
       "111  ValueCourier       build             0         NaN        0   \n",
       "112  ValueCourier       build             0         NaN        0   \n",
       "113  ValueCourier       build             0         NaN        0   \n",
       "\n",
       "    original_experiment original_session pairs ps4_events  session subject  \\\n",
       "0                   NaN                0   NaN        NaN        0  LTP229   \n",
       "1                   NaN                1   NaN        NaN        1  LTP229   \n",
       "2                   NaN                2   NaN        NaN        2  LTP229   \n",
       "3                   NaN                3   NaN        NaN        3  LTP229   \n",
       "4                   NaN                4   NaN        NaN        4  LTP229   \n",
       "..                  ...              ...   ...        ...      ...     ...   \n",
       "109                 NaN                4   NaN        NaN        4  LTP610   \n",
       "110                 NaN                5   NaN        NaN        5  LTP610   \n",
       "111                 NaN                0   NaN        NaN        0  LTP612   \n",
       "112                 NaN                0   NaN        NaN        0  LTP613   \n",
       "113                 NaN                0   NaN        NaN        0  LTP614   \n",
       "\n",
       "    subject_alias  system_version  \\\n",
       "0          LTP229             NaN   \n",
       "1          LTP229             NaN   \n",
       "2          LTP229             NaN   \n",
       "3          LTP229             NaN   \n",
       "4          LTP229             NaN   \n",
       "..            ...             ...   \n",
       "109        LTP610             NaN   \n",
       "110        LTP610             NaN   \n",
       "111        LTP612             NaN   \n",
       "112        LTP613             NaN   \n",
       "113        LTP614             NaN   \n",
       "\n",
       "                                           task_events  \n",
       "0    protocols/ltp/subjects/LTP229/experiments/VFFR...  \n",
       "1    protocols/ltp/subjects/LTP229/experiments/VFFR...  \n",
       "2    protocols/ltp/subjects/LTP229/experiments/VFFR...  \n",
       "3    protocols/ltp/subjects/LTP229/experiments/VFFR...  \n",
       "4    protocols/ltp/subjects/LTP229/experiments/VFFR...  \n",
       "..                                                 ...  \n",
       "109  protocols/ltp/subjects/LTP610/experiments/Valu...  \n",
       "110  protocols/ltp/subjects/LTP610/experiments/Valu...  \n",
       "111  protocols/ltp/subjects/LTP612/experiments/Valu...  \n",
       "112  protocols/ltp/subjects/LTP613/experiments/Valu...  \n",
       "113  protocols/ltp/subjects/LTP614/experiments/Valu...  \n",
       "\n",
       "[114 rows x 17 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_subjects = 10\n",
    "# experiments = [\"ValueCourier\", \"ltpFR\", \"ltpFR2\", \"VFFR\"]\n",
    "experiments = [\"VFFR\", \"ValueCourier\"]\n",
    "\n",
    "subjects_to_exclude = {\"LTP001\", \"LTP9992\", \"LTP9993\"}  # <-- your list here\n",
    "\n",
    "df = cml.get_data_index()\n",
    "\n",
    "df_exp = df[df[\"experiment\"].isin(experiments)].copy()\n",
    "\n",
    "# remove excluded subjects up front\n",
    "df_exp = df_exp[~df_exp[\"subject\"].isin(subjects_to_exclude)].copy()\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for exp in experiments:\n",
    "    df_this = df_exp[df_exp[\"experiment\"] == exp]\n",
    "\n",
    "    subjects = (\n",
    "        df_this[\"subject\"]\n",
    "        .drop_duplicates()\n",
    "        .sort_values()      # deterministic\n",
    "        .head(max_subjects)\n",
    "    )\n",
    "\n",
    "    df_keep = df_this[df_this[\"subject\"].isin(subjects)].copy()\n",
    "    dfs.append(df_keep)\n",
    "\n",
    "df_subset = pd.concat(dfs, ignore_index=True)\n",
    "df_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a6c17f29-cf09-47b5-9dca-81dcfaca67fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get futures\n",
    "bids_root = \"/home1/maint/LTP_BIDS/\"\n",
    "subjects = get_entity_vals(bids_root, \"subject\")\n",
    "out_path = \"raw_results_type/\"\n",
    "futures = []\n",
    "REL_START, REL_STOP = 200, 1000\n",
    "BUFFER_MS = 1000\n",
    "# evs_type = [\"WORD\"]\n",
    "evs_type = None\n",
    "tmin = (-BUFFER_MS)\n",
    "tmax = ((REL_STOP + BUFFER_MS))\n",
    "future_meta = {} \n",
    "futures_eeg = []\n",
    "for i, row in df_subset.iterrows():\n",
    "    sub = row[\"subject\"]\n",
    "    exp = row[\"experiment\"]\n",
    "    sess = row[\"session\"]\n",
    "    try:\n",
    "        process_epoched_signals_by_type(sub, exp, sess, evs_type, tmin, tmax, bids_root, out_path, verbose=True)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "#     fut = client.submit(\n",
    "#         process_epoched_signals_by_type,\n",
    "#         sub, exp, sess, evs_type, tmin, tmax, bids_root, out_path\n",
    "#     )\n",
    "\n",
    "#     futures_eeg.append(fut)\n",
    "#     future_meta[fut.key] = (sub, exp, sess)\n",
    "    # if i < 15:\n",
    "    #     break\n",
    "\n",
    "# for sub in subjects:\n",
    "#     subject_root = os.path.join(bids_root, f\"sub-{sub}\")\n",
    "#     experiments = get_entity_vals(subject_root, \"experiment\")\n",
    "#     print(experiments)\n",
    "#     sessions = get_entity_vals(subject_root, \"session\")\n",
    "#     # futures.extend([client.submit(process_raw_signals, sub, \"ValueCourier\", sess, bids_root,out_path) for sess in sessions])\n",
    "#     futures.extend([client.submit(process_epoched_signals, sub, exp, sess, evs_type, tmin, tmax, bids_root, out_path)for sess in sessions for exp in experiments])\n",
    "#     futures.extend([client.submit(process_events, sub, exp, sess, evs_type, bids_root, out_path) for sess in sessions])\n",
    "#     break\n",
    "#     # process_epoched_signals(sub, exp, sess, evs_types, tmin, tmax, bids_root, out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b4d99f01-8fb7-4233-99af-d6de3ccf40c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m all_df_time \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      8\u001b[0m n_done, n_fail \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fut \u001b[38;5;129;01min\u001b[39;00m as_completed(futures_eeg):\n\u001b[1;32m     11\u001b[0m     sub, exp, sess \u001b[38;5;241m=\u001b[39m future_meta\u001b[38;5;241m.\u001b[39mget(fut\u001b[38;5;241m.\u001b[39mkey, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<unknown>\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<unknown>\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<unknown>\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/global/miniconda/py310_23.1.0-1/envs/workshop_311/lib/python3.11/site-packages/distributed/client.py:5498\u001b[0m, in \u001b[0;36mas_completed.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5496\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m()\n\u001b[1;32m   5497\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthread_condition:\n\u001b[0;32m-> 5498\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthread_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5499\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_and_raise()\n",
      "File \u001b[0;32m/usr/global/miniconda/py310_23.1.0-1/envs/workshop_311/lib/python3.11/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# run futures \n",
    "from dask.distributed import as_completed\n",
    "\n",
    "all_df_raw = []\n",
    "all_df_raw_summary = []\n",
    "all_df_time = []\n",
    "\n",
    "n_done, n_fail = 0, 0\n",
    "\n",
    "for fut in as_completed(futures_eeg):\n",
    "    sub, exp, sess = future_meta.get(fut.key, (\"<unknown>\", \"<unknown>\", \"<unknown>\"))\n",
    "\n",
    "    try:\n",
    "        out = fut.result()\n",
    "\n",
    "        if out.get(\"skipped\"):\n",
    "            print(f\"[SKIP] {sub} | {exp} | {sess}\")\n",
    "            continue\n",
    "\n",
    "        if out[\"df_raw\"] is not None and not out[\"df_raw\"].empty:\n",
    "            all_df_raw.append(out[\"df_raw\"])\n",
    "\n",
    "        if out[\"df_raw_summary\"] is not None and not out[\"df_raw_summary\"].empty:\n",
    "            all_df_raw_summary.append(out[\"df_raw_summary\"])\n",
    "\n",
    "        if out[\"df_time\"] is not None and not out[\"df_time\"].empty:\n",
    "            all_df_time.append(out[\"df_time\"])\n",
    "\n",
    "        n_done += 1\n",
    "        print(f\"[DONE] {sub} | {exp} | {sess}  ({n_done})\")\n",
    "\n",
    "    except Exception as e:\n",
    "        n_fail += 1\n",
    "        print(f\"[FAIL] {sub} | {exp} | {sess}  -> {e}  ({n_fail})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025f8376-2f07-4557-9672-da8c6e966d6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "futures_beh = []\n",
    "evs_type = None\n",
    "for i, row in df_subset.iterrows():\n",
    "    sub = row[\"subject\"]\n",
    "    exp = row[\"experiment\"]\n",
    "    sess = row[\"session\"]\n",
    "    process_events(sub, exp, sess, evs_type, bids_root, out_path)\n",
    "    # futures_beh.append(client.submit(process_events, sub, exp, sess, evs_type, bids_root, out_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859ecba4-f06f-4bd4-a561-2a03a7091649",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "out_path = \"raw_results_type/\"\n",
    "\n",
    "df_raw_filenames = []\n",
    "df_raw_summary_filenames = []\n",
    "df_time_filenames = []\n",
    "df_behavior_summary_filenames = []\n",
    "\n",
    "for dirpath, _, filenames in os.walk(out_path):\n",
    "    for f in filenames:\n",
    "        full_path = os.path.join(dirpath, f)\n",
    "        \n",
    "        # Categorize based on string patterns\n",
    "        if f.startswith('df_raw_summary_') and f.endswith('.csv'):\n",
    "            df_raw_summary_filenames.append(full_path)\n",
    "        elif f.startswith('df_raw_') and f.endswith('.csv'):\n",
    "            df_raw_filenames.append(full_path)\n",
    "        elif f.startswith('df_time_') and f.endswith('.csv'):\n",
    "            df_time_filenames.append(full_path)\n",
    "        elif f.startswith('df_behavior_summary_') and f.endswith('.csv'):\n",
    "            df_behavior_summary_filenames.append(full_path)\n",
    "\n",
    "def load_and_concat(file_list, remove_duplicates=True):\n",
    "    \"\"\"\n",
    "    Load and concatenate CSV files with duplicate handling.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_list : list\n",
    "        List of file paths to concatenate\n",
    "    remove_duplicates : bool\n",
    "        Whether to remove duplicate rows (default: True)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Concatenated DataFrame with duplicates optionally removed\n",
    "    \"\"\"\n",
    "    if not file_list:\n",
    "        return pd.DataFrame()  # Return empty DF if no files found\n",
    "    \n",
    "    # Read each CSV and combine them into one, skipping empty files\n",
    "    dfs = []\n",
    "    for f in file_list:\n",
    "        try:\n",
    "            df = pd.read_csv(f)\n",
    "            if not df.empty:\n",
    "                dfs.append(df)\n",
    "            else:\n",
    "                print(f\"Warning: Skipping empty file: {f}\")\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"Warning: Skipping empty/malformed file: {f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error reading {f}: {e}\")\n",
    "    \n",
    "    if not dfs:\n",
    "        print(\"Warning: No valid CSV files found to concatenate\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    if remove_duplicates:\n",
    "        initial_rows = len(df)\n",
    "        df = df.drop_duplicates()\n",
    "        removed_rows = initial_rows - len(df)\n",
    "        if removed_rows > 0:\n",
    "            print(f\"Removed {removed_rows} duplicate rows\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def delete_source_files(file_list, delete_files=False):\n",
    "    \"\"\"\n",
    "    Delete source files after successful concatenation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_list : list\n",
    "        List of file paths to delete\n",
    "    delete_files : bool\n",
    "        Whether to actually delete the files (default: False for safety)\n",
    "    \"\"\"\n",
    "    if delete_files and file_list:\n",
    "        for f in file_list:\n",
    "            try:\n",
    "                os.remove(f)\n",
    "                print(f\"Deleted: {f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error deleting {f}: {e}\")\n",
    "\n",
    "# Configuration: Set to True to delete source files after concatenation\n",
    "DELETE_SOURCE_FILES = True  # Change to True when you're ready\n",
    "\n",
    "# Create the 4 distinct DataFrames\n",
    "print(\"Processing df_raw...\")\n",
    "df_raw_all = load_and_concat(df_raw_filenames)\n",
    "\n",
    "print(\"Processing df_raw_summary...\")\n",
    "df_raw_summary_all = load_and_concat(df_raw_summary_filenames)\n",
    "\n",
    "print(\"Processing df_time...\")\n",
    "df_time_all = load_and_concat(df_time_filenames)\n",
    "\n",
    "print(\"Processing df_behavior_summary...\")\n",
    "df_behavior_summary_all = load_and_concat(df_behavior_summary_filenames)\n",
    "\n",
    "# Save the concatenated files\n",
    "print(\"\\nSaving concatenated files...\")\n",
    "df_raw_all.to_csv(\"df_raw_all.csv\", index=False)\n",
    "df_raw_summary_all.to_csv(\"df_raw_summary_all.csv\", index=False)\n",
    "df_time_all.to_csv(\"df_time_all.csv\", index=False)\n",
    "df_behavior_summary_all.to_csv(\"df_behavior_summary_all.csv\", index=False)\n",
    "\n",
    "print(\"Concatenation complete!\")\n",
    "\n",
    "# Delete source files if configured\n",
    "if DELETE_SOURCE_FILES:\n",
    "    print(\"\\nDeleting source files...\")\n",
    "    delete_source_files(df_raw_filenames, DELETE_SOURCE_FILES)\n",
    "    delete_source_files(df_raw_summary_filenames, DELETE_SOURCE_FILES)\n",
    "    delete_source_files(df_time_filenames, DELETE_SOURCE_FILES)\n",
    "    delete_source_files(df_behavior_summary_filenames, DELETE_SOURCE_FILES)\n",
    "    print(\"Source file deletion complete!\")\n",
    "else:\n",
    "    print(\"\\nSource files retained (set DELETE_SOURCE_FILES=True to delete)\")\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Summary:\")\n",
    "print(f\"df_raw_all: {len(df_raw_all)} rows\")\n",
    "print(f\"df_raw_summary_all: {len(df_raw_summary_all)} rows\")\n",
    "print(f\"df_time_all: {len(df_time_all)} rows\")\n",
    "print(f\"df_behavior_summary_all: {len(df_behavior_summary_all)} rows\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d74ad98-284d-41a2-9a25-4a472b9df6a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load eeg\n",
    "df_raw_all = pd.read_csv(\"df_raw_all.csv\")\n",
    "df_raw_summary_all = pd.read_csv(\"df_raw_summary_all.csv\")\n",
    "df_time_all = pd.read_csv(\"df_time_all.csv\")\n",
    "df_behavior_summary_all = pd.read_csv(\"df_behavior_summary_all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90af5d4-22b0-499f-88d7-e8ceb9d5bae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot mean and std difference\n",
    "plot_comp_results(df_time_all, \"mean_abs_time_diff\", \"std_time_diff\", col_label=\"Mean Abs Time Diff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55f62f7-3975-4686-a4ef-7b987874a39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot mse\n",
    "plot_comp_results(df_time_all, \"mse_time\", col_label=\"MSE Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90e7bfd-5e91-4c13-9d31-660d6a625f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comp_results(df_raw_summary_all, \"mean_abs_diff\", \"std_diff\", col_label=\"Mean Abs Signal Diff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54d3cb3-ad83-4c9b-88de-09d3eb5042f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot mse\n",
    "plot_comp_results(df_raw_summary_all, \"mse\", col_label=\"MSE Raw Signal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ed4ed9-3ef6-4af8-bd59-7dc21f10457a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot n_channels diff\n",
    "plot_comp_results(df_raw_summary_all, \"n_exact_diff_channels\")\n",
    "plot_comp_results(df_raw_summary_all, \"n_close_diff_channels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23aacdc0-f9f4-486c-a382-d078dfe7b654",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comp_results(df_behavior_summary_all, \"n_differing_columns\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop_311",
   "language": "python",
   "name": "workshop_311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
